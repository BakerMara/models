SST2:
    lr: 0.000001    #1e-6
    batch_size: 32
    model_load_dir: '.'
    n_epochs: 30
    kwargs_path: './pretrain_model_SST-2/parameters.json'
    pretrain_dir: './pretrain_model_SST-2/weights'
    model_save_dir: './pretrain_model_SST2'
    task: 'SST-2'
    vocab_size: 30522
    type_vocab_size: 2
    max_position_embeddings: 512
    hidden_size: 768
    intermediate_size: 3072
    chunk_size_feed_forward: 0
    num_hidden_layers: 12
    num_attention_heads: 12
    hidden_act: "gelu"
    pad_token_id: 1
    layer_norm_eps: 0.000001 #1e-5
    attention_probs_dropout_prob: 0.1
    hidden_dropout_prob: 0.1
    position_embedding_type: "absolute"
    is_decoder: False
    add_pooling_layer: True
    add_cross_attention: False